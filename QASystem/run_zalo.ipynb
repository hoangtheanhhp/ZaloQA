{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "name": "run_zalo.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hoangtheanhhp/ZaloQA/blob/bert_lstm/QASystem/run_zalo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dh0YnmCRfuH",
        "colab_type": "code",
        "outputId": "dc17ccdf-d775-418c-f7ae-cfea4e3aa5c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9IxqDN4LPUQ",
        "colab_type": "code",
        "outputId": "f5f30e92-5ce8-4e3c-c73a-488d38012b9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "!rm -rf zalo\n",
        "!test -d zalo || git clone https://github.com/hoangtheanhhp/ZaloQA zalo\n",
        "import sys\n",
        "if 'zalo' not in sys.path:\n",
        "  sys.path+=['zalo']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'zalo'...\n",
            "remote: Enumerating objects: 100, done.\u001b[K\n",
            "remote: Counting objects:   1% (1/100)\u001b[K\rremote: Counting objects:   2% (2/100)\u001b[K\rremote: Counting objects:   3% (3/100)\u001b[K\rremote: Counting objects:   4% (4/100)\u001b[K\rremote: Counting objects:   5% (5/100)\u001b[K\rremote: Counting objects:   6% (6/100)\u001b[K\rremote: Counting objects:   7% (7/100)\u001b[K\rremote: Counting objects:   8% (8/100)\u001b[K\rremote: Counting objects:   9% (9/100)\u001b[K\rremote: Counting objects:  10% (10/100)\u001b[K\rremote: Counting objects:  11% (11/100)\u001b[K\rremote: Counting objects:  12% (12/100)\u001b[K\rremote: Counting objects:  13% (13/100)\u001b[K\rremote: Counting objects:  14% (14/100)\u001b[K\rremote: Counting objects:  15% (15/100)\u001b[K\rremote: Counting objects:  16% (16/100)\u001b[K\rremote: Counting objects:  17% (17/100)\u001b[K\rremote: Counting objects:  18% (18/100)\u001b[K\rremote: Counting objects:  19% (19/100)\u001b[K\rremote: Counting objects:  20% (20/100)\u001b[K\rremote: Counting objects:  21% (21/100)\u001b[K\rremote: Counting objects:  22% (22/100)\u001b[K\rremote: Counting objects:  23% (23/100)\u001b[K\rremote: Counting objects:  24% (24/100)\u001b[K\rremote: Counting objects:  25% (25/100)\u001b[K\rremote: Counting objects:  26% (26/100)\u001b[K\rremote: Counting objects:  27% (27/100)\u001b[K\rremote: Counting objects:  28% (28/100)\u001b[K\rremote: Counting objects:  29% (29/100)\u001b[K\rremote: Counting objects:  30% (30/100)\u001b[K\rremote: Counting objects:  31% (31/100)\u001b[K\rremote: Counting objects:  32% (32/100)\u001b[K\rremote: Counting objects:  33% (33/100)\u001b[K\rremote: Counting objects:  34% (34/100)\u001b[K\rremote: Counting objects:  35% (35/100)\u001b[K\rremote: Counting objects:  36% (36/100)\u001b[K\rremote: Counting objects:  37% (37/100)\u001b[K\rremote: Counting objects:  38% (38/100)\u001b[K\rremote: Counting objects:  39% (39/100)\u001b[K\rremote: Counting objects:  40% (40/100)\u001b[K\rremote: Counting objects:  41% (41/100)\u001b[K\rremote: Counting objects:  42% (42/100)\u001b[K\rremote: Counting objects:  43% (43/100)\u001b[K\rremote: Counting objects:  44% (44/100)\u001b[K\rremote: Counting objects:  45% (45/100)\u001b[K\rremote: Counting objects:  46% (46/100)\u001b[K\rremote: Counting objects:  47% (47/100)\u001b[K\rremote: Counting objects:  48% (48/100)\u001b[K\rremote: Counting objects:  49% (49/100)\u001b[K\rremote: Counting objects:  50% (50/100)\u001b[K\rremote: Counting objects:  51% (51/100)\u001b[K\rremote: Counting objects:  52% (52/100)\u001b[K\rremote: Counting objects:  53% (53/100)\u001b[K\rremote: Counting objects:  54% (54/100)\u001b[K\rremote: Counting objects:  55% (55/100)\u001b[K\rremote: Counting objects:  56% (56/100)\u001b[K\rremote: Counting objects:  57% (57/100)\u001b[K\rremote: Counting objects:  58% (58/100)\u001b[K\rremote: Counting objects:  59% (59/100)\u001b[K\rremote: Counting objects:  60% (60/100)\u001b[K\rremote: Counting objects:  61% (61/100)\u001b[K\rremote: Counting objects:  62% (62/100)\u001b[K\rremote: Counting objects:  63% (63/100)\u001b[K\rremote: Counting objects:  64% (64/100)\u001b[K\rremote: Counting objects:  65% (65/100)\u001b[K\rremote: Counting objects:  66% (66/100)\u001b[K\rremote: Counting objects:  67% (67/100)\u001b[K\rremote: Counting objects:  68% (68/100)\u001b[K\rremote: Counting objects:  69% (69/100)\u001b[K\rremote: Counting objects:  70% (70/100)\u001b[K\rremote: Counting objects:  71% (71/100)\u001b[K\rremote: Counting objects:  72% (72/100)\u001b[K\rremote: Counting objects:  73% (73/100)\u001b[K\rremote: Counting objects:  74% (74/100)\u001b[K\rremote: Counting objects:  75% (75/100)\u001b[K\rremote: Counting objects:  76% (76/100)\u001b[K\rremote: Counting objects:  77% (77/100)\u001b[K\rremote: Counting objects:  78% (78/100)\u001b[K\rremote: Counting objects:  79% (79/100)\u001b[K\rremote: Counting objects:  80% (80/100)\u001b[K\rremote: Counting objects:  81% (81/100)\u001b[K\rremote: Counting objects:  82% (82/100)\u001b[K\rremote: Counting objects:  83% (83/100)\u001b[K\rremote: Counting objects:  84% (84/100)\u001b[K\rremote: Counting objects:  85% (85/100)\u001b[K\rremote: Counting objects:  86% (86/100)\u001b[K\rremote: Counting objects:  87% (87/100)\u001b[K\rremote: Counting objects:  88% (88/100)\u001b[K\rremote: Counting objects:  89% (89/100)\u001b[K\rremote: Counting objects:  90% (90/100)\u001b[K\rremote: Counting objects:  91% (91/100)\u001b[K\rremote: Counting objects:  92% (92/100)\u001b[K\rremote: Counting objects:  93% (93/100)\u001b[K\rremote: Counting objects:  94% (94/100)\u001b[K\rremote: Counting objects:  95% (95/100)\u001b[K\rremote: Counting objects:  96% (96/100)\u001b[K\rremote: Counting objects:  97% (97/100)\u001b[K\rremote: Counting objects:  98% (98/100)\u001b[K\rremote: Counting objects:  99% (99/100)\u001b[K\rremote: Counting objects: 100% (100/100)\u001b[K\rremote: Counting objects: 100% (100/100), done.\u001b[K\n",
            "remote: Compressing objects: 100% (77/77), done.\u001b[K\n",
            "remote: Total 616 (delta 58), reused 54 (delta 22), pack-reused 516\u001b[K\n",
            "Receiving objects: 100% (616/616), 94.28 MiB | 23.35 MiB/s, done.\n",
            "Resolving deltas: 100% (286/286), done.\n",
            "Checking out files: 100% (40/40), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sdx1bGwe4GnQ",
        "colab_type": "code",
        "outputId": "c3ddf0cc-2a13-4da3-e384-7c968a5e3680",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!pip install tensorflow==1.15.2\n",
        "!pip install kashgari==1.1.4\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Requirement already satisfied: tensorflow==1.15.2 in /tensorflow-1.15.2/python3.6 (1.15.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (3.10.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (0.8.1)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /tensorflow-1.15.2/python3.6 (from tensorflow==1.15.2) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /tensorflow-1.15.2/python3.6 (from tensorflow==1.15.2) (1.15.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (0.2.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.28.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (3.2.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.12.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.12.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.0.8)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.16.4)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (0.9.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (0.34.2)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (0.2.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.15.2) (46.1.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.2.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (1.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.2) (2.10.0)\n",
            "Requirement already satisfied: kashgari==1.1.4 in /usr/local/lib/python3.6/dist-packages (1.1.4)\n",
            "Requirement already satisfied: keras-bert>=0.50.0 in /usr/local/lib/python3.6/dist-packages (from kashgari==1.1.4) (0.81.0)\n",
            "Requirement already satisfied: keras-gpt-2>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from kashgari==1.1.4) (0.14.0)\n",
            "Requirement already satisfied: scikit-learn>=0.21.1 in /usr/local/lib/python3.6/dist-packages (from kashgari==1.1.4) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy==1.16.4 in /usr/local/lib/python3.6/dist-packages (from kashgari==1.1.4) (1.16.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from kashgari==1.1.4) (2.10.0)\n",
            "Requirement already satisfied: seqeval==0.0.10 in /usr/local/lib/python3.6/dist-packages (from kashgari==1.1.4) (0.0.10)\n",
            "Requirement already satisfied: gensim>=3.5.0 in /usr/local/lib/python3.6/dist-packages (from kashgari==1.1.4) (3.6.0)\n",
            "Requirement already satisfied: bert4keras==0.6.5 in /usr/local/lib/python3.6/dist-packages (from kashgari==1.1.4) (0.6.5)\n",
            "Requirement already satisfied: pandas>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from kashgari==1.1.4) (1.0.3)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-bert>=0.50.0->kashgari==1.1.4) (2.3.1)\n",
            "Requirement already satisfied: keras-transformer>=0.30.0 in /usr/local/lib/python3.6/dist-packages (from keras-bert>=0.50.0->kashgari==1.1.4) (0.32.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from keras-gpt-2>=0.8.0->kashgari==1.1.4) (2019.12.20)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.1->kashgari==1.1.4) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.1->kashgari==1.1.4) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->kashgari==1.1.4) (1.12.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.5.0->kashgari==1.1.4) (1.11.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23.0->kashgari==1.1.4) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23.0->kashgari==1.1.4) (2.8.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert>=0.50.0->kashgari==1.1.4) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert>=0.50.0->kashgari==1.1.4) (1.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert>=0.50.0->kashgari==1.1.4) (3.13)\n",
            "Requirement already satisfied: keras-pos-embd>=0.10.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.30.0->keras-bert>=0.50.0->kashgari==1.1.4) (0.11.0)\n",
            "Requirement already satisfied: keras-position-wise-feed-forward>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.30.0->keras-bert>=0.50.0->kashgari==1.1.4) (0.6.0)\n",
            "Requirement already satisfied: keras-embed-sim>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.30.0->keras-bert>=0.50.0->kashgari==1.1.4) (0.7.0)\n",
            "Requirement already satisfied: keras-multi-head>=0.22.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.30.0->keras-bert>=0.50.0->kashgari==1.1.4) (0.22.0)\n",
            "Requirement already satisfied: keras-layer-normalization>=0.12.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.30.0->keras-bert>=0.50.0->kashgari==1.1.4) (0.14.0)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.4) (2.49.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.4) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.4) (1.12.40)\n",
            "Requirement already satisfied: keras-self-attention==0.41.0 in /usr/local/lib/python3.6/dist-packages (from keras-multi-head>=0.22.0->keras-transformer>=0.30.0->keras-bert>=0.50.0->kashgari==1.1.4) (0.41.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.4) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.4) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.4) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.4) (2.8)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.4) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.4) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.40 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.4) (1.15.40)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.40->boto3->smart-open>=1.2.1->gensim>=3.5.0->kashgari==1.1.4) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtVpXIw6M4CO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATASET_PATH = \"zalo/Dataset/zalo\"\n",
        "SAVE_MODEL_PATH = \"/content/drive/My Drive/ZaloAI/result/modelCNN_BiLSTM\"\n",
        "MODEL_PATH = \"/content/drive/My Drive/ZaloAI/bert\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfjKr9Z_d9RR",
        "colab_type": "code",
        "outputId": "e48dd28a-9069-4803-ee51-79ba01416e56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from zalo.QASystem.preprocess import ZaloDatasetProcessor\n",
        "\n",
        "import kashgari\n",
        "processor = ZaloDatasetProcessor()\n",
        "from kashgari.embeddings import BERTEmbedding\n",
        "\n",
        "bert_embedding = BERTEmbedding(MODEL_PATH,\n",
        "                               task=kashgari.CLASSIFICATION,\n",
        "                               sequence_length=128)\n",
        "\n",
        "tokenizer = bert_embedding.tokenizer"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:seq_len: 128\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNSYwQrNPQgY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Our tokenizer already added the BOS([CLS]) and EOS([SEP]) token\n",
        "# so we need to disable the default add_bos_eos setting.\n",
        "bert_embedding.processor.add_bos_eos = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvMGadO85XIX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "processor.val_data = []\n",
        "processor.train_data = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSZxqz_vRPze",
        "colab_type": "code",
        "outputId": "f128b2c5-b8e2-4b37-cb47-c4ca68be4ac0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "processor.load_from_path(DATASET_PATH,\"train\", \"train.json\")\n",
        "processor.load_from_path(DATASET_PATH,\"val\", \"val.json\")\n",
        "\n",
        "data = processor.train_data\n",
        "val = processor.val_data\n",
        "train_x = []\n",
        "train_y = []\n",
        "val_x = []\n",
        "val_y = []\n",
        "for d in data:\n",
        "  train_x.append(tokenizer.tokenize(d.get('question')) + \n",
        "                 tokenizer.tokenize(d.get('text'))[1:])\n",
        "  train_y.append(str(d.get('label', False)))\n",
        "\n",
        "for d in val:\n",
        "  val_x.append(tokenizer.tokenize(d.get('question')) + \n",
        "               tokenizer.tokenize(d.get('text'))[1:])\n",
        "  val_y.append(str(d.get('label', False)))\n",
        "\n",
        "print(len(train_x), len(train_y), len(val_x), len(val_y))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 14200/14200 [00:00<00:00, 1183513.17it/s]\n",
            "100%|██████████| 3906/3906 [00:00<00:00, 719497.21it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "14200 14200 3906 3906\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsTR3HcAtthR",
        "colab_type": "code",
        "outputId": "ea5e4a08-6276-46e4-f358-408a54d4e3cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from tensorflow.python import keras\n",
        "from kashgari.tasks.classification import BiLSTM_Model\n",
        "from kashgari.callbacks import EvalCallBack\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level='DEBUG')\n",
        "\n",
        "model = BiLSTM_Model()\n",
        "\n",
        "model.fit(train_x,\n",
        "          train_y,\n",
        "          val_x,\n",
        "          val_y,\n",
        "          epochs=100,\n",
        "          batch_size=32)\n",
        "\n",
        "model.save(SAVE_MODEL_PATH)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_12\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Input-Token (InputLayer)        [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Input-Segment (InputLayer)      [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token (TokenEmbedding [(None, 128, 768), ( 81315072    Input-Token[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Segment (Embedding)   (None, 128, 768)     1536        Input-Segment[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token-Segment (Add)   (None, 128, 768)     0           Embedding-Token[0][0]            \n",
            "                                                                 Embedding-Segment[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Position (PositionEmb (None, 128, 768)     98304       Embedding-Token-Segment[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Dropout (Dropout)     (None, 128, 768)     0           Embedding-Position[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Norm (LayerNormalizat (None, 128, 768)     1536        Embedding-Dropout[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 128, 768)     2362368     Embedding-Norm[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-1-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 128, 768)     0           Embedding-Norm[0][0]             \n",
            "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 128, 768)     1536        Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward (FeedForw (None, 128, 768)     4722432     Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Dropout ( (None, 128, 768)     0           Encoder-1-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Add (Add) (None, 128, 768)     0           Encoder-1-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Norm (Lay (None, 128, 768)     1536        Encoder-1-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 128, 768)     2362368     Encoder-1-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-2-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-1-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 128, 768)     1536        Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward (FeedForw (None, 128, 768)     4722432     Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Dropout ( (None, 128, 768)     0           Encoder-2-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Add (Add) (None, 128, 768)     0           Encoder-2-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-2-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Norm (Lay (None, 128, 768)     1536        Encoder-2-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 128, 768)     2362368     Encoder-2-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-3-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-2-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 128, 768)     1536        Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward (FeedForw (None, 128, 768)     4722432     Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Dropout ( (None, 128, 768)     0           Encoder-3-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Add (Add) (None, 128, 768)     0           Encoder-3-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-3-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Norm (Lay (None, 128, 768)     1536        Encoder-3-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 128, 768)     2362368     Encoder-3-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-4-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-3-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 128, 768)     1536        Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward (FeedForw (None, 128, 768)     4722432     Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Dropout ( (None, 128, 768)     0           Encoder-4-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Add (Add) (None, 128, 768)     0           Encoder-4-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-4-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Norm (Lay (None, 128, 768)     1536        Encoder-4-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 128, 768)     2362368     Encoder-4-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-5-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-4-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 128, 768)     1536        Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward (FeedForw (None, 128, 768)     4722432     Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Dropout ( (None, 128, 768)     0           Encoder-5-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Add (Add) (None, 128, 768)     0           Encoder-5-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-5-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Norm (Lay (None, 128, 768)     1536        Encoder-5-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 128, 768)     2362368     Encoder-5-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-6-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-5-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 128, 768)     1536        Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward (FeedForw (None, 128, 768)     4722432     Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Dropout ( (None, 128, 768)     0           Encoder-6-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Add (Add) (None, 128, 768)     0           Encoder-6-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-6-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Norm (Lay (None, 128, 768)     1536        Encoder-6-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 128, 768)     2362368     Encoder-6-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-7-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-6-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-7-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 128, 768)     1536        Encoder-7-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward (FeedForw (None, 128, 768)     4722432     Encoder-7-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Dropout ( (None, 128, 768)     0           Encoder-7-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Add (Add) (None, 128, 768)     0           Encoder-7-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-7-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Norm (Lay (None, 128, 768)     1536        Encoder-7-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 128, 768)     2362368     Encoder-7-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-8-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-7-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-8-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 128, 768)     1536        Encoder-8-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward (FeedForw (None, 128, 768)     4722432     Encoder-8-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Dropout ( (None, 128, 768)     0           Encoder-8-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Add (Add) (None, 128, 768)     0           Encoder-8-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-8-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Norm (Lay (None, 128, 768)     1536        Encoder-8-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 128, 768)     2362368     Encoder-8-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-9-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-8-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-9-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 128, 768)     1536        Encoder-9-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward (FeedForw (None, 128, 768)     4722432     Encoder-9-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Dropout ( (None, 128, 768)     0           Encoder-9-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Add (Add) (None, 128, 768)     0           Encoder-9-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-9-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Norm (Lay (None, 128, 768)     1536        Encoder-9-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 128, 768)     2362368     Encoder-9-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 128, 768)     0           Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 128, 768)     0           Encoder-9-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 128, 768)     1536        Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward (FeedFor (None, 128, 768)     4722432     Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Dropout  (None, 128, 768)     0           Encoder-10-FeedForward[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Add (Add (None, 128, 768)     0           Encoder-10-MultiHeadSelfAttention\n",
            "                                                                 Encoder-10-FeedForward-Dropout[0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Norm (La (None, 128, 768)     1536        Encoder-10-FeedForward-Add[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 128, 768)     2362368     Encoder-10-FeedForward-Norm[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 128, 768)     0           Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 128, 768)     0           Encoder-10-FeedForward-Norm[0][0]\n",
            "                                                                 Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 128, 768)     1536        Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward (FeedFor (None, 128, 768)     4722432     Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Dropout  (None, 128, 768)     0           Encoder-11-FeedForward[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Add (Add (None, 128, 768)     0           Encoder-11-MultiHeadSelfAttention\n",
            "                                                                 Encoder-11-FeedForward-Dropout[0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Norm (La (None, 128, 768)     1536        Encoder-11-FeedForward-Add[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 128, 768)     2362368     Encoder-11-FeedForward-Norm[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 128, 768)     0           Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 128, 768)     0           Encoder-11-FeedForward-Norm[0][0]\n",
            "                                                                 Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 128, 768)     1536        Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward (FeedFor (None, 128, 768)     4722432     Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Dropout  (None, 128, 768)     0           Encoder-12-FeedForward[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Add (Add (None, 128, 768)     0           Encoder-12-MultiHeadSelfAttention\n",
            "                                                                 Encoder-12-FeedForward-Dropout[0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Norm (La (None, 128, 768)     1536        Encoder-12-FeedForward-Add[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-Output (Concatenate)    (None, 128, 3072)    0           Encoder-9-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-10-FeedForward-Norm[0][0]\n",
            "                                                                 Encoder-11-FeedForward-Norm[0][0]\n",
            "                                                                 Encoder-12-FeedForward-Norm[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "non_masking_layer_1 (NonMasking (None, 128, 3072)    0           Encoder-Output[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv1d (Conv1D)                 (None, 128, 32)      294944      non_masking_layer_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D)    (None, 64, 32)       0           conv1d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   (None, 100)          53200       max_pooling1d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 2)            202         lstm_3[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 166,819,258\n",
            "Trainable params: 348,346\n",
            "Non-trainable params: 166,470,912\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.5846 - acc: 0.7008Epoch 1/100\n",
            "71/71 [==============================] - 112s 2s/step - loss: 0.5840 - acc: 0.7013 - val_loss: 0.5814 - val_acc: 0.6969\n",
            "Epoch 2/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.5567 - acc: 0.7189Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.5573 - acc: 0.7187 - val_loss: 0.5566 - val_acc: 0.7268\n",
            "Epoch 3/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.5236 - acc: 0.7429Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.5238 - acc: 0.7431 - val_loss: 0.4938 - val_acc: 0.7739\n",
            "Epoch 4/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.4867 - acc: 0.7694Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.4871 - acc: 0.7692 - val_loss: 0.4740 - val_acc: 0.7755\n",
            "Epoch 5/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.4674 - acc: 0.7801Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.4671 - acc: 0.7803 - val_loss: 0.4300 - val_acc: 0.8116\n",
            "Epoch 6/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.4268 - acc: 0.8100Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.4264 - acc: 0.8101 - val_loss: 0.4113 - val_acc: 0.8100\n",
            "Epoch 7/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.4103 - acc: 0.8131Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.4107 - acc: 0.8129 - val_loss: 0.3513 - val_acc: 0.8454\n",
            "Epoch 8/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.3768 - acc: 0.8313Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.3767 - acc: 0.8313 - val_loss: 0.3269 - val_acc: 0.8615\n",
            "Epoch 9/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.3582 - acc: 0.8408Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.3584 - acc: 0.8406 - val_loss: 0.3015 - val_acc: 0.8766\n",
            "Epoch 10/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.3416 - acc: 0.8496Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.3408 - acc: 0.8500 - val_loss: 0.2681 - val_acc: 0.8968\n",
            "Epoch 11/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.3192 - acc: 0.8633Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.3202 - acc: 0.8630 - val_loss: 0.2828 - val_acc: 0.8914\n",
            "Epoch 12/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.2988 - acc: 0.8755Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.2989 - acc: 0.8756 - val_loss: 0.2342 - val_acc: 0.9112\n",
            "Epoch 13/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.2855 - acc: 0.8796Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.2857 - acc: 0.8794 - val_loss: 0.2266 - val_acc: 0.9117\n",
            "Epoch 14/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.2742 - acc: 0.8842Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.2734 - acc: 0.8846 - val_loss: 0.2015 - val_acc: 0.9275\n",
            "Epoch 15/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.2515 - acc: 0.8964Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.2520 - acc: 0.8964 - val_loss: 0.2050 - val_acc: 0.9268\n",
            "Epoch 16/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.2399 - acc: 0.9006Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.2399 - acc: 0.9006 - val_loss: 0.1788 - val_acc: 0.9357\n",
            "Epoch 17/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.2333 - acc: 0.9025Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.2339 - acc: 0.9021 - val_loss: 0.1870 - val_acc: 0.9316\n",
            "Epoch 18/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.2233 - acc: 0.9090Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.2236 - acc: 0.9090 - val_loss: 0.1521 - val_acc: 0.9439\n",
            "Epoch 19/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.2113 - acc: 0.9147Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.2109 - acc: 0.9147 - val_loss: 0.1399 - val_acc: 0.9493\n",
            "Epoch 20/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.1948 - acc: 0.9216Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.1955 - acc: 0.9215 - val_loss: 0.1125 - val_acc: 0.9626\n",
            "Epoch 21/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.1913 - acc: 0.9226Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.1907 - acc: 0.9228 - val_loss: 0.1259 - val_acc: 0.9508\n",
            "Epoch 22/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.1847 - acc: 0.9256Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.1846 - acc: 0.9255 - val_loss: 0.1042 - val_acc: 0.9639\n",
            "Epoch 23/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.1823 - acc: 0.9273Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.1826 - acc: 0.9273 - val_loss: 0.1929 - val_acc: 0.9181\n",
            "Epoch 24/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.1759 - acc: 0.9268Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.1752 - acc: 0.9270 - val_loss: 0.0899 - val_acc: 0.9672\n",
            "Epoch 25/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.1672 - acc: 0.9318Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.1680 - acc: 0.9316 - val_loss: 0.0844 - val_acc: 0.9757\n",
            "Epoch 26/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.1616 - acc: 0.9354Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.1618 - acc: 0.9353 - val_loss: 0.0749 - val_acc: 0.9759\n",
            "Epoch 27/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.1588 - acc: 0.9366Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.1583 - acc: 0.9367 - val_loss: 0.0827 - val_acc: 0.9739\n",
            "Epoch 28/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.1485 - acc: 0.9391Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.1488 - acc: 0.9389 - val_loss: 0.0709 - val_acc: 0.9752\n",
            "Epoch 29/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.1497 - acc: 0.9414Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.1496 - acc: 0.9415 - val_loss: 0.0864 - val_acc: 0.9736\n",
            "Epoch 30/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.1464 - acc: 0.9417Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.1461 - acc: 0.9418 - val_loss: 0.0730 - val_acc: 0.9780\n",
            "Epoch 31/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.1359 - acc: 0.9469Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.1351 - acc: 0.9471 - val_loss: 0.0605 - val_acc: 0.9795\n",
            "Epoch 32/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.1371 - acc: 0.9445Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.1375 - acc: 0.9445 - val_loss: 0.0595 - val_acc: 0.9813\n",
            "Epoch 33/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.1350 - acc: 0.9464Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.1352 - acc: 0.9465 - val_loss: 0.0558 - val_acc: 0.9821\n",
            "Epoch 34/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.1325 - acc: 0.9487Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.1332 - acc: 0.9485 - val_loss: 0.0726 - val_acc: 0.9790\n",
            "Epoch 35/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.1253 - acc: 0.9536Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.1254 - acc: 0.9535 - val_loss: 0.0574 - val_acc: 0.9816\n",
            "Epoch 36/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.1216 - acc: 0.9532Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.1226 - acc: 0.9528 - val_loss: 0.0654 - val_acc: 0.9813\n",
            "Epoch 37/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.1184 - acc: 0.9547Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.1182 - acc: 0.9547 - val_loss: 0.0497 - val_acc: 0.9869\n",
            "Epoch 38/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.1183 - acc: 0.9540Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.1181 - acc: 0.9539 - val_loss: 0.0425 - val_acc: 0.9892\n",
            "Epoch 39/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.1170 - acc: 0.9544Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.1171 - acc: 0.9545 - val_loss: 0.0386 - val_acc: 0.9885\n",
            "Epoch 40/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.1099 - acc: 0.9571Epoch 1/100\n",
            "71/71 [==============================] - 100s 1s/step - loss: 0.1099 - acc: 0.9571 - val_loss: 0.0629 - val_acc: 0.9811\n",
            "Epoch 41/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.1072 - acc: 0.9593Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.1075 - acc: 0.9592 - val_loss: 0.0586 - val_acc: 0.9772\n",
            "Epoch 42/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.1045 - acc: 0.9581Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.1047 - acc: 0.9581 - val_loss: 0.0384 - val_acc: 0.9872\n",
            "Epoch 43/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.1029 - acc: 0.9607Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.1029 - acc: 0.9606 - val_loss: 0.0404 - val_acc: 0.9846\n",
            "Epoch 44/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.1100 - acc: 0.9569Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.1098 - acc: 0.9569 - val_loss: 0.0360 - val_acc: 0.9875\n",
            "Epoch 45/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.0985 - acc: 0.9629Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.0985 - acc: 0.9628 - val_loss: 0.0340 - val_acc: 0.9890\n",
            "Epoch 46/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.1008 - acc: 0.9626Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.1010 - acc: 0.9625 - val_loss: 0.0356 - val_acc: 0.9905\n",
            "Epoch 47/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.0979 - acc: 0.9627Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.0988 - acc: 0.9624 - val_loss: 0.0425 - val_acc: 0.9890\n",
            "Epoch 48/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.0938 - acc: 0.9640Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.0938 - acc: 0.9638 - val_loss: 0.0475 - val_acc: 0.9844\n",
            "Epoch 49/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.0944 - acc: 0.9641Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.0953 - acc: 0.9637 - val_loss: 0.0551 - val_acc: 0.9798\n",
            "Epoch 50/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.0944 - acc: 0.9641Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.0941 - acc: 0.9641 - val_loss: 0.0455 - val_acc: 0.9859\n",
            "Epoch 51/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.0984 - acc: 0.9614Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.0978 - acc: 0.9616 - val_loss: 0.0359 - val_acc: 0.9895\n",
            "Epoch 52/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.0962 - acc: 0.9619Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.0959 - acc: 0.9621 - val_loss: 0.0287 - val_acc: 0.9916\n",
            "Epoch 53/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.0839 - acc: 0.9685Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.0842 - acc: 0.9685 - val_loss: 0.0263 - val_acc: 0.9913\n",
            "Epoch 54/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.0957 - acc: 0.9637Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.0956 - acc: 0.9639 - val_loss: 0.0301 - val_acc: 0.9923\n",
            "Epoch 55/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.0793 - acc: 0.9694Epoch 1/100\n",
            "71/71 [==============================] - 100s 1s/step - loss: 0.0794 - acc: 0.9694 - val_loss: 0.0243 - val_acc: 0.9951\n",
            "Epoch 56/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.0889 - acc: 0.9667Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.0888 - acc: 0.9666 - val_loss: 0.0320 - val_acc: 0.9913\n",
            "Epoch 57/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.0861 - acc: 0.9663Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.0866 - acc: 0.9661 - val_loss: 0.0337 - val_acc: 0.9890\n",
            "Epoch 58/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.0829 - acc: 0.9694Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.0830 - acc: 0.9692 - val_loss: 0.0228 - val_acc: 0.9954\n",
            "Epoch 59/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.0870 - acc: 0.9670Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.0862 - acc: 0.9672 - val_loss: 0.0200 - val_acc: 0.9962\n",
            "Epoch 60/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.0816 - acc: 0.9695Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.0821 - acc: 0.9694 - val_loss: 0.0393 - val_acc: 0.9885\n",
            "Epoch 61/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.0807 - acc: 0.9694Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.0804 - acc: 0.9695 - val_loss: 0.0329 - val_acc: 0.9918\n",
            "Epoch 62/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.0801 - acc: 0.9686Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.0798 - acc: 0.9687 - val_loss: 0.0252 - val_acc: 0.9933\n",
            "Epoch 63/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.0786 - acc: 0.9710Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.0788 - acc: 0.9711 - val_loss: 0.0263 - val_acc: 0.9913\n",
            "Epoch 64/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.0914 - acc: 0.9658Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.0910 - acc: 0.9659 - val_loss: 0.0199 - val_acc: 0.9939\n",
            "Epoch 65/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.0755 - acc: 0.9721Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.0756 - acc: 0.9721 - val_loss: 0.0217 - val_acc: 0.9921\n",
            "Epoch 66/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.0740 - acc: 0.9740Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.0740 - acc: 0.9739 - val_loss: 0.0220 - val_acc: 0.9939\n",
            "Epoch 67/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.0727 - acc: 0.9733Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.0730 - acc: 0.9732 - val_loss: 0.0313 - val_acc: 0.9910\n",
            "Epoch 68/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.0759 - acc: 0.9723Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.0759 - acc: 0.9723 - val_loss: 0.0176 - val_acc: 0.9954\n",
            "Epoch 69/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.0745 - acc: 0.9721Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.0743 - acc: 0.9720 - val_loss: 0.0215 - val_acc: 0.9949\n",
            "Epoch 70/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.0777 - acc: 0.9715Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.0781 - acc: 0.9715 - val_loss: 0.0220 - val_acc: 0.9939\n",
            "Epoch 71/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.0718 - acc: 0.9721Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.0722 - acc: 0.9721 - val_loss: 0.0194 - val_acc: 0.9954\n",
            "Epoch 72/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.0747 - acc: 0.9726Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.0749 - acc: 0.9724 - val_loss: 0.0463 - val_acc: 0.9862\n",
            "Epoch 73/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.0714 - acc: 0.9737Epoch 1/100\n",
            "71/71 [==============================] - 100s 1s/step - loss: 0.0711 - acc: 0.9738 - val_loss: 0.0232 - val_acc: 0.9931\n",
            "Epoch 74/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.0692 - acc: 0.9734Epoch 1/100\n",
            "71/71 [==============================] - 100s 1s/step - loss: 0.0692 - acc: 0.9735 - val_loss: 0.0149 - val_acc: 0.9967\n",
            "Epoch 75/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.0711 - acc: 0.9732Epoch 1/100\n",
            "71/71 [==============================] - 100s 1s/step - loss: 0.0716 - acc: 0.9729 - val_loss: 0.0168 - val_acc: 0.9967\n",
            "Epoch 76/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.0718 - acc: 0.9729Epoch 1/100\n",
            "71/71 [==============================] - 100s 1s/step - loss: 0.0723 - acc: 0.9729 - val_loss: 0.0153 - val_acc: 0.9974\n",
            "Epoch 77/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.0702 - acc: 0.9736Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.0707 - acc: 0.9735 - val_loss: 0.0149 - val_acc: 0.9956\n",
            "Epoch 78/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.0749 - acc: 0.9711Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.0751 - acc: 0.9711 - val_loss: 0.0269 - val_acc: 0.9921\n",
            "Epoch 79/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.0665 - acc: 0.9752Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.0669 - acc: 0.9751 - val_loss: 0.0259 - val_acc: 0.9913\n",
            "Epoch 80/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.0645 - acc: 0.9772Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.0645 - acc: 0.9772 - val_loss: 0.0141 - val_acc: 0.9967\n",
            "Epoch 81/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.0649 - acc: 0.9763Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.0646 - acc: 0.9764 - val_loss: 0.0134 - val_acc: 0.9964\n",
            "Epoch 82/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.0719 - acc: 0.9721Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.0723 - acc: 0.9719 - val_loss: 0.0149 - val_acc: 0.9964\n",
            "Epoch 83/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.0641 - acc: 0.9757Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.0642 - acc: 0.9758 - val_loss: 0.0141 - val_acc: 0.9974\n",
            "Epoch 84/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.0687 - acc: 0.9746Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.0688 - acc: 0.9745 - val_loss: 0.0154 - val_acc: 0.9967\n",
            "Epoch 85/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.0634 - acc: 0.9775Epoch 1/100\n",
            "71/71 [==============================] - 100s 1s/step - loss: 0.0633 - acc: 0.9775 - val_loss: 0.0217 - val_acc: 0.9933\n",
            "Epoch 86/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.0588 - acc: 0.9773Epoch 1/100\n",
            "71/71 [==============================] - 100s 1s/step - loss: 0.0590 - acc: 0.9772 - val_loss: 0.0129 - val_acc: 0.9964\n",
            "Epoch 87/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.0656 - acc: 0.9768Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.0666 - acc: 0.9766 - val_loss: 0.0149 - val_acc: 0.9972\n",
            "Epoch 88/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.0621 - acc: 0.9781Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.0618 - acc: 0.9782 - val_loss: 0.0151 - val_acc: 0.9962\n",
            "Epoch 89/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.0678 - acc: 0.9743Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.0677 - acc: 0.9744 - val_loss: 0.0150 - val_acc: 0.9959\n",
            "Epoch 90/100\n",
            "70/71 [============================>.] - ETA: 1s - loss: 0.0625 - acc: 0.9766Epoch 1/100\n",
            "71/71 [==============================] - 99s 1s/step - loss: 0.0625 - acc: 0.9766 - val_loss: 0.0123 - val_acc: 0.9972\n",
            "Epoch 91/100\n",
            "29/71 [===========>..................] - ETA: 49s - loss: 0.0575 - acc: 0.9774"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i--cVMr7fTXf",
        "colab_type": "code",
        "outputId": "a3b9fc80-4a13-4b69-bf84-ece522376841",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from kashgari.utils import load_model\n",
        "\n",
        "model = load_model(SAVE_MODEL_PATH)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Sequence length will auto set at 95% of sequence length\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3q3YYXzS4Esr",
        "colab_type": "code",
        "outputId": "11c231cd-b3f8-42c1-c9e9-430e6312db01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "processor.load_from_path(\"/content/zalo/Dataset/zalo\", \"test\",\"test_full.json\")\n",
        "test = processor.test_data\n",
        "test_x = []\n",
        "test_y = []\n",
        "for d in test:\n",
        "  test_x.append(tokenizer.tokenize(d.get('question')) + \n",
        "               tokenizer.tokenize(d.get('text'))[1:])\n",
        "  test_y.append(str(d.get('label')))\n",
        "# evaluate the model\n",
        "model.evaluate(test_x, test_y)\n",
        "# print(\"loss: %f\\n accruracy: %f\\nf1_score: %f\\nprecision: %f\\n, recall: %f\\n:\" \n",
        "#       % (loss, accurany,f1_score,precision, recall))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 688/688 [00:00<00:00, 164061.69it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False     0.8960    0.8728    0.8842      3128\n",
            "        True     0.1367    0.1658    0.1498       380\n",
            "\n",
            "    accuracy                         0.7962      3508\n",
            "   macro avg     0.5163    0.5193    0.5170      3508\n",
            "weighted avg     0.8137    0.7962    0.8047      3508\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}